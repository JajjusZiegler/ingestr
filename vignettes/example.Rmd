---
title: "Examples for ingestr"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r setup, include=FALSE}
library(ingestr)
library(tidyverse)
```

# Overview

The package `ingestr` provides functions to extract (ingest) environmental point data (given longitude, latitude, and required dates) from large global files or remote data servers and create time series at user-specified temporal resolution (currently, just daily implemented). 

- Temporal downscaling from montly to daily resolution
- Quality filtering, temporal interpolation and smoothing of remote sensing data
- Handling of different APIs and file formats, returning ingested data in *tidy* format.

This is to make your life simpler when downloading and reading site-scale data, using a common interface with a single function for single-site and multi-site ingest, respectively, and a common and tidy format of ingested data across a variety of data *sources* and formats of original files. *Sources*, refers to both data sets hosted remotely and accessed through an API and local data sets. ingestr is particularly suited for preparing model forcing and offers a set of functionalities to transform original data into common standardized formats and units. This includes interpolation methods for converting monthly climate data (CRU TS currently) to daily time steps. 

The key functions are `ingest_bysite()` and `ingest()` for a single-site data ingest and a multi-site data ingest, respectively. For the multi-site data ingest, site meta information is provided through the argument `siteinfo` which takes a data frame with columns `lon` for longitude, `lat` for latitude, and (for time series downloads) `year_start` and `year_end`, specifying required dates (including all days of respective years). Sites are organised along rows. An example site meta info data frame is provided as part of this package for sites included in the FLUXNET2015 Tier 1 data set (additional columns are not required by `ingest_bysite()` and `ingest()`):
```{r}
siteinfo_fluxnet2015 %>% 
  slice(1:5) %>% 
  knitr::kable()
```

The following *sources* can be handled currently:

| Data source                                                          | Data type                                | Coverage | Source ID     | Reading from  | Remark     |
|-------------------------                                             |---------------                           |--------- |---------------| ---           |---         |
| [FLUXNET](https://fluxnet.fluxdata.org/data/fluxnet2015-dataset/)    | time series by site   | site     | `fluxnet`     | local files   |            |
| [WATCH-WFDEI](http://www.eu-watch.org/data_availability)             | time series raster map                                    | global   | `watch_wfdei` | local files   |            |
| [CRU](https://crudata.uea.ac.uk/cru/data/hrg/)                       | time series raster map                                    | global   | `cru`         | local files   |            |
| MODIS LP DAAC                                                        | time series raster map                           | global   | `modis`       | remote server | using [MODISTools](https://docs.ropensci.org/MODISTools/) |
| Google Earth Engine                                                  | time series raster map                          | global   | `gee`         | remote server | using Koen Hufken's [gee_suset](https://khufkens.github.io/gee_subset/) library |
| [ETOPO1](https://www.ngdc.noaa.gov/mgg/global/)                      | raster map                                | global   | `etopo1`      | local files   |            |
| [Mauna Loa CO2](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html) | time series                        | site     | `co2_mlo`     | remote server | using the [climate](https://github.com/bczernecki/climate) R package |
| HWSD                                                                 | shapefile map                                     | global   | `hwsd`        | local files   | using an adaption of David Le Bauer's [rhwsd](https://github.com/dlebauer/rhwsd) R package |
| [WWF Ecoregions](https://databasin.org/datasets/68635d7c77f1475f9b6c1d1dbe0a4c4c) | shapefile map   | global   | `wwf`         | local files   | Olsen et al. (2001)| 
| [N deposition](https://link.springer.com/article/10.1007%2Fs10584-011-0155-0)    | time series raster map   | global     | `ndep`     | local files   | Lamarque et al. (2011) |

Examples to read data for a single site for each data type are given in Section 'Examples for a single site'. Handling ingestion for multiple sites is described in Section 'Example for a set of sites'.
**Note** that this package does not provide the original data. Please follow links to data sources above where data is read from local files, and always cite original references.

## Variable names and units

All ingested data follows standardized variable naming and (optionally) units. 

| Variable                           | Variable name | Units                          |
|-------------------------           |---------------|---------------                 |
| Gross primary production           | `gpp`         | g CO$^{-2}$ m$^{-2}$           |
| Air temperature                    | `temp`        | $^\circ$C                      |
| Daily minimum air temperature      | `tmin`        | $^\circ$C                      |
| Daily maximum air temperature      | `tmax`        | $^\circ$C                      |
| Precipitation                      | `prec`        | mm s$^{-1}$                    |
| Vapour pressure deficit            | `vpd`         | Pa                             |
| Atmospheric pressure               | `patm`        | Pa                             |
| Net radiation                      | `netrad`      | J m$^{-2}$ s$^{-1}=$ W m$^{-2}$|
| Photosynthetic photon flux density | `ppfd`        | mol m$^{-2}$ s$^{-1}$          |
| Elevation (altitude)               | `elv`         | m a.s.l.                       |        

Use these variable names for specifying which variable names they correspond to in the original data source (see argument `getvars` to functions `ingest()` and `ingest_bysite()`). `gpp` is cumulative, corresponding to the time scale of the data. For example, if daily data is read, `gpp` is the total gross primary production per day (g CO$^{-2}$ m$^{-2}$ d$^{-1}$).


# Examples for a single site

The function `ingest_bysite()` can be used to ingest data for a single site. The argument `source` specifies which data type (*source*) is to be read from and triggers the use of specific wrapper functions that are designed to read from original files with formats that differ between sources. Source-specific settings for data processing can be provided by argument `settings` (described for each data source below). More info about other, source-independent arguments are available through the man page (see `?ingest_bysite`).

## FLUXNET

### Meteo data

Reading from FLUXNET files offers multiple settings to be used specified by the user. Here, we're specifying that no soil water content data is read (`getswc = FALSE` in `settings_fluxnet`, passed to `ingest_bysite()` through argument `settings`).
```{r message=FALSE}
settings_fluxnet <- list(getswc = FALSE)

df_fluxnet <- ingest_bysite(
  sitename = "FR-Pue",
  source = "fluxnet",
  getvars = list(temp = "TA_F",
                 prec = "P_F",
                 vpd  = "VPD_F",
                 ppfd =  "SW_IN_F",
                 netrad = "NETRAD",
                 patm = "PA_F"),
  dir = paste0(path.package("ingestr"), "/extdata/"),  # example file delivered through package and located here
  settings = settings_fluxnet,
  timescale = "d",
  year_start = 2007,
  year_end = 2007,
  verbose = FALSE
  )
df_fluxnet
```

`getvars` defines the variable names in the original files corresponding to the respective variables with ingestr-standard naming (see table above). The example above triggers the ingestion of the six variables `"TA_F", "P_F", "VPD_F",  "SW_IN_F", "NETRAD", "PA_F"` for `"temp", "prec", "vpd", "ppfd", "netrad", "patm"`, respectively.

### Flux data

The same function can also be used to read in other FLUXNET variables (e.g., CO2 flux data) and conduct data filtering steps. Here, we're reading daily GPP and uncertainty (standard error), based on the nighttime flux decomposition method (`"GPP_NT_VUT_REF"` and `"GPP_NT_VUT_SE"` in argument `getvars`). The `settings` argument can be used again to specify settings that are specific to the `"fluxnet"` data source. Here, we keep only data where at least 80% is based on non-gapfilled half-hourly data (`threshold_GPP = 0.8`), and where the daytime and nighttime-based estimates are consistent, that is, where their difference is below the the 97.5% and above the 2.5% quantile (`filter_ntdt = TRUE`). Negative GPP values are not removed (`remove_neg = FALSE`). We read data for just one year here (2007).
```{r warning=FALSE, message=FALSE}
settings_fluxnet <- list(
  getswc       = FALSE,
  filter_ntdt  = TRUE,
  threshold_GPP= 0.8,
  remove_neg   = FALSE
  )

ddf_fluxnet <- ingest_bysite(
  sitename  = "FR-Pue",
  source    = "fluxnet",
  getvars   = list( gpp = "GPP_NT_VUT_REF",
                    gpp_unc = "GPP_NT_VUT_SE"),
  dir       = paste0(path.package("ingestr"), "/extdata/"),
  settings  = settings_fluxnet,
  timescale = "d",
  year_start = 2007,
  year_end  = 2007
  )
```


### Settings

The argument `settings` in functions `ingest_bysite()` and `ingest()` is used to pass settings that are specific to the data source (argument `source`) with which the functions are used. Default settings are specified for each data source. For `source = "fluxnet"`, defaults are returned by a function call of `get_settings_fluxnet()` and are described in the function's man page (see `?get_settings_fluxnet`). Defaults are used for settings elements that are not specified by the user.

## WATCH-WFDEI

Let's extract data for the location corresponding to FLUXNET site 'CH-Lae' (lon = 8.365, lat = 47.4781). This extracts from original WATCH-WFDEI files, provided as NetCDF (global, 0.5 degree resolution), provided as monthly files containing all days in each month. The data directory specified here (`dir = "~/data/watch_wfdei/"`) contains sub-directories with names containing the variable names. The argument `getvars` works a differently compared to `"fluxnet"`. Here, `getvars` is a vector of ingestr-standard variable names to be read. ingestr automatically reads from the respective files with WATCH-WFDEI variable names. Available variables are: `"temp", "ppfd", "vpd", "patm", "prec"`. The latter is the sum of snow and rain. Below, we read data for just one year here (2007).
```{r message=FALSE, echo = T, results = 'hide'}
df_watch <- ingest_bysite(
  sitename  = "FR-Pue",
  source    = "watch_wfdei",
  getvars   = c("temp"),
  dir       = "~/data/watch_wfdei/",
  timescale = "d",
  year_start = 2007,
  year_end  = 2007,
  lon       = 3.5958,
  lat       = 43.7414,
  verbose   = FALSE
  )
df_watch
```

## CRU TS

As above, let's extract CRU data for the location corresponding to FLUXNET site 'FR-Pue' (lon = 8.365, lat = 47.4781). The argument `getvars` works the same way as for WATCH-WFDEI: is a vector of ingestr-standard variable names to be read. ingestr automatically reads from the respective files with CRU variable names. Available variables are: `"temp", "prec", "vpd", "ccov", "wetd"`.

Note that we're using `tmx` (the daily maximum temperature). This extracts monthly data from the CRU TS data. Interpolation to daily values is done using a wather generator for daily precipitation (given monthly total precipitation and number of wet days in each month), and a polynomial that conserves monthly means for all other variables.
```{r message=FALSE}
df_cru <- ingest_bysite(
  sitename  = "FR-Pue",
  source    = "cru",
  getvars   = c("tmin", "tmax"),
  dir       = "~/data/cru/ts_4.01/",
  timescale = "d",
  year_start = 2007,
  year_end  = 2007,
  lon       = 3.5958,
  lat       = 43.7414,
  verbose   = FALSE
  )
df_cru
```

We can compare the temperature recorded at the site and the temperature data extracted from WATCH-WFDEI and CRU.
```{r}
df <- df_fluxnet %>%
  rename(temp_fluxnet = temp) %>%
  left_join(rename(df_watch, temp_watch = temp), by = c("sitename", "date")) %>%
  left_join(rename(df_cru, temp_min_cru = tmin, temp_max_cru = tmax), by = c("sitename", "date")) %>%
  pivot_longer(cols = c(temp_fluxnet, temp_watch, temp_min_cru, temp_max_cru), names_to = "source", values_to = "temp", names_prefix = "temp_")

library(ggplot2)
df %>%
  ggplot(aes(x = date, y = temp, color = source)) +
  geom_line()
```

Looks sweet.

## MODIS LP DAAC

This uses the [MODISTools](https://docs.ropensci.org/MODISTools/) R package making its interface consistent with ingestr. Settings can be specified and passed on using the `settings` argument. To facilitate the selection of data products and bands to be downloaded, you may use the function `get_settings_modis)` which defines defaults for different data bundles (`c("modis_fpar", "modis_ndvi", "modis_evi")` are available).

- `"modis_fpar"`: MODIS collection 6, MCD15A3H, band `Fpar_500m`
- `"modis_lai"`: MODIS collection 6, MCD15A3H, band `Lai_500m`
- `"modis_evi"`: MODIS collection 6, MOD13Q1, band `250m_16_days_EVI`
- `"modis_ndvi"`: MODIS collection 6, MOD13Q1, band `250m_16_days_NDVI`

The filtering criteria are hard-coded specifically for each product, using its respective quality control information (see function `gapfill_interpol()` in `R/ingest_modis_bysite.R`).

The following example is for downloading MODIS NDVI data.
```{r}
settings_modis <- get_settings_modis(
  bundle            = "modis_lai",
  data_path         = "~/data/modis_subsets/",
  method_interpol   = "loess",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= TRUE
  )
```

This can now be used to download the data to the directory specified by argument `data_path` of function `get_settings_gee()`.
```{r warning=FALSE}
df_modis_fpar <- ingest_bysite(
  sitename  = "CH-Lae",
  source    = "modis",
  year_start= 2018,
  year_end  = 2019,
  lon       = 8.36439,
  lat       = 47.47833,
  settings  = settings_modis,
  verbose   = FALSE
  )
```

Plot this data.
```{r}
plot_fapar_ingestr_bysite(
  df_modis_fpar, 
  settings_modis)
```


<!-- ```{r} -->
<!-- df <- read_csv("~/data/modis_subsets/raw/MODIS_NDVI_MOD13Q1_CH-Lae.csv") -->

<!-- df_ndvi_spatialmean <- df %>% -->
<!--   dplyr::filter(band == "250m_16_days_NDVI") %>%  -->
<!--   mutate(calendar_date = ymd(calendar_date)) %>%  -->
<!--   group_by(calendar_date) %>% -->
<!--   summarise(mean = mean(value), min = min(value), max = max(value)) %>% -->
<!--   mutate(mean = mean * scale_factor, min = min * scale_factor, max = max * scale_factor) -->

<!-- df_ndvi_spatialmean %>%  -->
<!--   ggplot(aes(x = calendar_date)) + -->
<!--   geom_ribbon(aes(ymin = min, ymax = max), fill = "grey70") + -->
<!--   geom_line(aes(y = mean)) -->

<!-- df_modis_fpar %>%  -->
<!--   ggplot() + -->
<!--   geom_point(aes(date, modisvar_filtered)) + -->
<!--   geom_line(aes(date, linear)) -->
<!-- ``` -->


## Google Earth Engine

The library `gee_subset` by Koen Hufkens can be downloaded from this [link](https://khufkens.github.io/gee_subset/) and used to extract data directly from Google Earth Engine. Note that this requires the following programmes to be available:

- git: You can use [Homebrew](https://brew.sh/) to installing git by entering in your terminal: `brew install git`.
- [python](https://www.python.org/)
- The Python Pandas library

Then, carry out the follwing steps:

- In your terminal, change to where you want to have the repository. In this example, we're cloning it into our home directory:
```{sh, eval = FALSE}
cd ~
git clone https://github.com/khufkens/google_earth_engine_subsets.git
```

To get access to using the Google Earth Engine API (required to use the `gee_subset` library), carry out the following steps in your terminal. This follows steps described [here](https://github.com/google/earthengine-api/issues/27).

1. Install google API Python client
```{sh, eval = FALSE}
sudo pip install --upgrade google-api-python-client
```
I had an error and first had to do this here following [this link](https://github.com/pypa/pip/issues/3165):
```{sh, eval = FALSE}
sudo pip install --ignore-installed six
```

2. Install pyCrypto
```{sh, eval = FALSE}
sudo pip install pyCrypto --upgrade
```

3. Install Python GEE API
```{sh, eval = FALSE}
sudo pip install earthengine-api
```

4. Run authentification for GEE
```{sh, eval = FALSE}
earthengine authenticate
```

5. Finally, try if it works. This shouldn't return an error:
```{sh, eval = FALSE}
python -c "import ee; ee.Initialize()"
```


### MODIS FPAR

To facilitate the selection of data products and bands to be downloaded, you may use the function `get_settings_gee()` which defines defaults for different data bundles (`c("modis_fpar", "modis_evi", "modis_lai", "modis_gpp")` are available).

- `"modis_fpar"`: MODIS/006/MCD15A3H, band Fpar
- `"modis_evi"`: MODIS/006/MOD13Q1, band EVI
- `"modis_lai"`: MOD15A2, band `Lai_1km`
- `"modis_gpp"`: MODIS/006/MOD17A2H, band Gpp

The following example is for downloading MODIS FPAR data.
```{r}
settings_gee <- get_settings_gee(
  bundle            = "modis_fpar",
  python_path       = system("which python", intern = TRUE),
  gee_path          = "~/google_earth_engine_subsets/gee_subset/",
  data_path         = "~/data/gee_subsets/",
  method_interpol   = "linear",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= TRUE
  )
```

This can now be used to download the data to the directory specified by argument `data_path` of function `get_settings_gee()`.
```{r}
df_gee_modis_fpar <- ingest_bysite(
  sitename  = "CH-Lae",
  source    = "gee",
  year_start= 2009,
  year_end  = 2010,
  lon       = 8.36439,
  lat       = 47.47833,
  settings  = settings_gee,
  verbose   = FALSE
  )
```

Plot this data.
```{r}
plot_fapar_ingestr_bysite(df_gee_modis_fpar, settings_gee)
```

## CO2

Ingesting CO2 data is particularly simple. We can safely assume it's well mixed in the atmosphere (independent of site location), and we can use a annual mean value for all days in respective years, and use the same value for all
sites. Using the R package [climate](https://github.com/bczernecki/climate), we can load CO2 data from Mauna Loa directly into R. This is downloading data from [ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt](ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt). Here, `ingest()` is a wrapper for the function `climate::meteo_noaa_co2()`.
```{r message=FALSE}
df_co2 <- ingest_bysite(
  sitename  = "CH-Lae",
  source  = "co2_mlo",
  year_start= 2007,
  year_end  = 2014,
  verbose = FALSE
  )
```

More info can be obtained [here](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html) and by:
```{r, eval=FALSE}
?climate::meteo_noaa_co2
```

## HWSD

Four steps are required before you can use `ingest_bysite()` to get HWSD data:

1. The the modified version of David LeBauer's [rhwsd](https://github.com/dlebauer/rhwsd) R package. The modified version can be installed by:
```{r, eval=FALSE}
if(!require(devtools)){install.packages(devtools)}
devtools::install_github("stineb/rhwsd")
```
2. Install additionally required packages: DBI and RSQLite.
```{r}
list.of.packages <- c("DBI", "RSQLite")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```
3. Download the HWSD data file [HWSD_RASTER.zip](http://webarchive.iiasa.ac.at/Research/LUC/External-World-soil-database/HWSD_Data/HWSD_RASTER.zip) and extract.
4. Move the extracted files to a local directory and adjust the file path in the `settings` argument accordingly (in this example: `"~/data/hwsd/HWSD_RASTER/hwsd.bil"`).

Then, use similarly to above, with providing the path to the downloaded file with the `settings` argument:
```{r, eval = FALSE}
df_hwsd <- ingest_bysite(
  sitename  = "CH-Lae",
  source  = "hwsd",
  lon     = 8.36439,
  lat     = 47.47833,
  settings = list(fil = "~/data/hwsd/HWSD_RASTER/hwsd.bil"),
  verbose = FALSE
  )
```


## Nitrogen deposition

This reads nitrogen deposition from global annual maps by Lamarque et al. (2011). This provides annual data separately for NHx and NOy in gN m$^{-2}$ yr$^{-1}$ from a global map provided at half-degree resolution and covering years 1860-2009. 
```{r, eval = FALSE}
df_ndep <- ingest_bysite(
  sitename  = "CH-Lae",
  source    = "ndep",
  lon       = 8.36439,
  lat       = 47.47833,
  year_start= 2000,
  year_end  = 2009,
  timescale = "y",
  dir       = "~/data/ndep_lamarque/",
  verbose   = FALSE
  )
```

# Examples for a site ensemble

To collect data from an ensemble of sites, we have to define a meta data frame, here called `siteinfo`, with rows for each site and columns `lon` for longitude, `lat` for latitude, `date_start` and `date_end` for required dates (Dates are objects returned by a `lubridate::ymd()` function call - this stands for year-month-day). The function `ingest()` can then be used to collect all site-level data as a nested data frame corresponding to the metadata `siteinfo` with an added column named `data` where the time series of ingested data is nested inside.

Note that extracting for an ensemble of sites at once is more efficient for data types that are global files (WATCH-WFDEI, and CRU). In this case, the `raster` package can be used to efficiently ingest data.

First, define a list of sites and get site meta information. The required meta information is provided by the exported data frame `siteinfo` (it comes as part of the ingestr package). This file is created as described in (and using code from) [metainfo_fluxnet2015](https://github.com/stineb/metainfo_fluxnet2015).
```{r warning=FALSE, message=FALSE}
mysites <- c("BE-Vie", "DE-Tha", "DK-Sor", "FI-Hyy", "IT-Col", "NL-Loo", "US-MMS", "US-WCr", "US-UMB", "US-Syv", "DE-Hai")

siteinfo <- ingestr::siteinfo_fluxnet2015 %>%
  dplyr::filter(sitename %in% mysites) %>%
  dplyr::mutate(date_start = lubridate::ymd(paste0(year_start, "-01-01"))) %>%
  dplyr::mutate(date_end = lubridate::ymd(paste0(year_end, "-12-31")))
```

This file looks like this:
```{r}
print(siteinfo)
```

Next, the data can be ingested for all sites at once. Let's do it for different data types again.

## FLUXNET

### Meteo data

This ingests meteorological data from the FLUXNET files for variables temperature, precipitation, VPD, shortwave incoming radiation, net radiation, and atmospheric pressure. Arguments that are specific for this data source are provided in the `settings` list.
```{r message=FALSE, warning=FALSE}
ddf_fluxnet <- ingest(
  siteinfo  = siteinfo,
  source    = "fluxnet",
  getvars   = list(temp = "TA_F", prec = "P_F", vpd  = "VPD_F", ppfd =  "SW_IN_F", netrad = "NETRAD", patm = "PA_F"),
  dir       = "~/data/FLUXNET-2015_Tier1/20191024/DD/",  # adjust this with your local path
  settings  = list(
    dir_hh = "~/data/FLUXNET-2015_Tier1/20191024/HH/", # adjust this with your local path
    getswc = FALSE),
  timescale = "d",
  verbose = TRUE
  )
```

### Flux data

As described above for a single site, the same function can also be used to read in other FLUXNET variables (e.g., CO2 flux data) and conduct data filtering steps. Here, we're reading daily GPP and uncertainty (standard error), based on the nighttime flux decomposition method (`""GPP_NT_VUT_REF""`), keep only data where at least 80% is based on non-gapfilled half-hourly data (`threshold_GPP = 0.8`), and where the daytime and nighttime-based estimates are consistent, that is, where their difference is below the the 97.5% and above the 2.5% quantile (`filter_ntdt = TRUE`, see also `?get_obs_bysite_fluxnet2015`).
```{r warning=FALSE, message=FALSE}
settings_fluxnet <- list(
  getswc       = FALSE,
  filter_ntdt  = TRUE,
  threshold_GPP= 0.8,
  remove_neg   = FALSE
  )

ddf_fluxnet_gpp <- ingest(
  siteinfo = siteinfo,
  source   = "fluxnet",
  getvars  = list(gpp = "GPP_NT_VUT_REF",
  pp_unc   = "GPP_NT_VUT_SE"),
  dir      = "~/data/FLUXNET-2015_Tier1/20191024/DD/", # adjust this with your local path
  settings = settings_fluxnet,
  timescale= "d"
  )
```

## WATCH-WFDEI

This extracts from original WATCH-WFDEI files, provided as NetCDF (global, 0.5 degree resolution), provided as monthly files containing all days in each month. The data directory specified here (`dir = "~/data/watch_wfdei/"`) contains subdirectories with names containing the variable names (corresponding to the ones specified by the argument `getvars = list(temp = "Tair")`).
```{r echo = T, results = 'hide'}
ddf_watch <- ingest(
  siteinfo = siteinfo %>% slice(1:3),
  source    = "watch_wfdei",
  getvars   = c("temp"),
  dir       = "~/data/watch_wfdei/"  # adjust this with your local path
  )
```

## CRU TS

This extracts monthly data from the CRU TS data. Interpolation to daily values is done using a wather generator for daily precipitation (given monthly total precipitation and number of wet days in each month), and a polynomial that conserves monthly means for all other variables.
```{r message=FALSE}
ddf_cru <- ingest(
  siteinfo = siteinfo %>% slice(1:2),
  source    = "cru",
  getvars   = c("tmax"),
  dir       = "~/data/cru/ts_4.01/"  # adjust this with your local path
  )
```

Check it out for the first site (BE-Vie).
```{r}
ggplot() +
  geom_line(data = ddf_fluxnet$data[[1]], aes(x = date, y = temp)) +
  geom_line(data = ddf_watch$data[[1]], aes(x = date, y = temp), col = "royalblue") +
  geom_line(data = ddf_cru$data[[1]], aes(x = date, y = tmax), col = "red") +
  xlim(ymd("2000-01-01"), ymd("2005-12-31"))
```

## MODIS LP DAAC

This uses the [MODISTools](https://docs.ropensci.org/MODISTools/) R package making its interface consistent with ingestr. Settings can be specified and passed on using the `settings` argument. To facilitate the selection of data products and bands to be downloaded, you may use the function `get_settings_modis)` which defines defaults for different data bundles (`c("modis_fpar", "modis_ndvi", "modis_evi")` are available).

- `"modis_fpar"`: MODIS collection 6, MCD15A3H, band `Fpar_500m`
- `"modis_lai"`: MODIS collection 6, MCD15A3H, band `Lai_500m`
- `"modis_evi"`: MODIS collection 6, MOD13Q1, band `250m_16_days_EVI`
- `"modis_ndvi"`: MODIS collection 6, MOD13Q1, band `250m_16_days_NDVI`

The filtering criteria are hard-coded specifically for each product, using its respective quality control information (see function `gapfill_interpol()` in `R/ingest_modis_bysite.R`).

Downloading with parallel jobs is available for the `"modis"` data ingest, using the package [multidplyr](https://github.com/tidyverse/multidplyr). This is not (yet) available on CRAN, but can be installed with `devtools::install_github("tidyverse/multidplyr")`. To do parallel downloading, set the following arguments in the function `ingest()`: `parallel = TRUE, ncores = <number_of_parallel_jobs>`.

The following example is for downloading MODIS NDVI data.
```{r}
settings_modis <- get_settings_modis(
  bundle            = "modis_ndvi",
  data_path         = "~/data/modis_subsets/",
  method_interpol   = "loess",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= TRUE
  )
```

This can now be used to download the data to the directory specified by argument `data_path` of function `get_settings_gee()`.
```{r warning=FALSE, message=FALSE}
df_modis_fpar <- ingest(
  siteinfo_fluxnet2015 %>% slice(1:3), 
  source = "modis",
  settings = settings_modis, 
  parallel = FALSE,
  ncores = 2
  )
```

Plot this data.
```{r}
plot_fapar_ingestr_bysite(
  df_modis_fpar$data[[1]] %>% 
    dplyr::filter(year(date) %in% 2010:2015), 
  settings_modis)

plot_fapar_ingestr_bysite(
  df_modis_fpar$data[[2]] %>% 
    dplyr::filter(year(date) %in% 2010:2015), 
  settings_modis)

plot_fapar_ingestr_bysite(
  df_modis_fpar$data[[3]] %>% 
    dplyr::filter(year(date) %in% 2010:2015), 
  settings_modis)
```



## Google Earth Engine

Using the same settings as specified above, we can download MODIS FPAR data for multiple sites at once from GEE:
```{r warning=FALSE, message=FALSE, echo = T, results = 'hide'}
settings_gee <- get_settings_gee(
  bundle            = "modis_fpar",
  python_path       = system("which python", intern = TRUE),
  gee_path          = "~/google_earth_engine_subsets/gee_subset/",    # adjust this with your local path
  data_path         = "~/data/gee_subsets/",    # adjust this with your local path
  method_interpol   = "linear",
  keep              = TRUE,
  overwrite_raw     = FALSE,
  overwrite_interpol= TRUE
  )

df_gee_modis_fpar <- ingest(
  siteinfo= siteinfo,
  source  = "gee",
  settings= settings_gee,
  verbose = FALSE
  )
```

Collect all plots.
```{r warning=FALSE, message=FALSE}
list_gg <- plot_fapar_ingestr(df_gee_modis_fpar, settings_gee)
#purrr::map(list_gg, ~print(.))
```

## CO2

Ingesting CO2 data is particularly simple. We can safely assume it's well mixed in the atmosphere (independent of site location), and we can use a annual mean value for all days in respective years, and use the same value for all
sites. Using the R package [climate](https://github.com/bczernecki/climate), we can load CO2 data from Mauna Loa directly into R. This is downloading data from [ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt](ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt). Here, `ingest()` is a wrapper for the function `climate::meteo_noaa_co2()`.
```{r message=FALSE}
df_co2 <- ingest(
  siteinfo,
  source  = "co2_mlo",
  verbose = FALSE
  )
```

More info can be obtained [here](https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html) and by:
```{r, eval=FALSE}
?climate::meteo_noaa_co2
```

## ETOPO1

This reads from the 1 arc minutes resolution ETOPO1 global elevation data (reading from a Geo-TIFF file). The nested data column contains a tibble one value for variable `elv`. Download the data from [here](https://www.ngdc.noaa.gov/mgg/global/) and specify the local path with the argument `dir`.
```{r}
df_etopo <- ingest(
  siteinfo,
  source = "etopo1",
  dir = "~/data/etopo/"  # adjust this with your local path
)
```

## HWSD

Four steps are required before you can use `ingest()` to get HWSD data:

1. The the modified version of David LeBauer's [rhwsd](https://github.com/dlebauer/rhwsd) R package. The modified version can be installed by:
```{r, eval=FALSE}
if(!require(devtools)){install.packages(devtools)}
devtools::install_github("stineb/rhwsd")
```
2. Install additionally required packages: DBI and RSQLite.
```{r, eval=FALSE}
list.of.packages <- c("DBI", "RSQLite")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```
3. Download the HWSD data file [HWSD_RASTER.zip](http://webarchive.iiasa.ac.at/Research/LUC/External-World-soil-database/HWSD_Data/HWSD_RASTER.zip) and extract.
4. Move the extracted files to a local directory and adjust the file path in the `settings` argument accordingly (in this example: `"~/data/hwsd/HWSD_RASTER/hwsd.bil"`).

Then, use similarly to above, with providing the path to the downloaded file with the `settings` argument:
```{r, eval = FALSE}
df_hwsd <- ingest(
  siteinfo,
  source = "hwsd",
  settings = list(fil = "~/data/hwsd/HWSD_RASTER/hwsd.bil")
  )
```

## WWF Ecoregions

WWF Ecoregions data are provided as a shapefile, available for download [here](http://assets.worldwildlife.org/publications/15/files/original/official_teow.zip), or [here](https://databasin.org/datasets/68635d7c77f1475f9b6c1d1dbe0a4c4c). A description of the data is available [here](http://omap.africanmarineatlas.org/BIOSPHERE/data/note_areas_sp/Ecoregions_Ecosystems/WWF_Ecoregions/WWFecoregions.htm). Download the zipped directory and adjust the argument `dir` to the path of the directory where file `wwf_terr_ecos.shp` is located. Set the settings list with `layer = "wwf_terr_ecos"`. Then, ingest data by:
```{r}
df_wwf <- ingest(
  siteinfo,
  source = "wwf",
  dir = "~/data/biomes/wwf_ecoregions/official/",
  settings = list(layer = "wwf_terr_ecos")
)
```

The following provides the biome codes. This information is additionally added by the ingestr package in column `BIOME_NAME`:

| Code | Biome |
|------|-------|
| 1    | Tropical & Subtropical Moist Broadleaf Forests |
| 2    | Tropical & Subtropical Dry Broadleaf Forests |
| 3    | Tropical & Subtropical Coniferous Forests |
| 4    | Temperate Broadleaf & Mixed Forests |
| 5    | Temperate Conifer Forests |
| 6    | Boreal Forests/Taiga |
| 7    | Tropical & Subtropical Grasslands, Savannas & Shrublands |
| 8    | Temperate Grasslands, Savannas & Shrublands |
| 9    | Flooded Grasslands & Savannas |
| 10   | Montane Grasslands & Shrublands |
| 11   | Tundra |
| 12   | Mediterranean Forests, Woodlands & Scrub |
| 13   | Deserts & Xeric Shrublands |
| 14   | Mangroves |

Please cite as:
*Olson, D. M., Dinerstein, E. ,Wikramanayake, E. D., Burgess, N. D., Powel, G. V. N., Underwood, E. C., Damico, J. A., Itoua, I., Strand, H. E., Morrison, J. C., Loucks, C. J., Ricketts, T. H., Kura, Y., Lamoreux, J. F., Wettengel, W. W., Hedao, P., and Kassem, K.R. 2001 Terrestrial ecoregions of the world: A new map of life on earth. BioScience, 51(11):933–938.*


## Nitrogen deposition

This reads nitrogen deposition from global annual maps by Lamarque et al. (2011). This provides annual data separately for NHx and NOy in gN m$^{-2}$ yr$^{-1}$ from a global map provided at half-degree resolution and covering years 1860-2009. 
```{r, eval = FALSE}
df_ndep <- ingest(
  siteinfo %>% slice(1:2),
  source    = "ndep",
  timescale = "y",
  dir       = "~/data/ndep_lamarque/",
  verbose   = FALSE
  )
```
